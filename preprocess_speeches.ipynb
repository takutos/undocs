{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Python code for extracting and preprocessing UNSC speech transpripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procedures for speech extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing and cleaning UNSC speech records\n",
    "import re\n",
    "\n",
    "def clean_PV(path, form=\"pdf\"):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    - path (string): path to an input plain text file (converted from either MS doc file or PDF)\n",
    "    - form (string): format of the original file obtained from UN Document system ('doc': MS Word, 'pdf': PDF)\n",
    "    \n",
    "    Returns:\n",
    "    - president (tupl): the counbcil's president name ('string') and country ('string')\n",
    "    - agenda (string): meeting agenda\n",
    "    - text (string): cleaned text\n",
    "    - num_words (integer): # of words contained in the text\n",
    "    - num_paras (integer): # of paragraphs contained in the text\n",
    "    \"\"\"\n",
    "    \n",
    "    f = open(path, 'r')\n",
    "    lines = f.readlines()\n",
    "    num_lines = len(lines)\n",
    "    main = False\n",
    "    search_president = True\n",
    "    search_agenda = [True, False]\n",
    "    agenda_para = 0\n",
    "    pdf_correction1 = [True, False]\n",
    "    text_to_move = []\n",
    "    pdf_correction2 = False\n",
    "    \n",
    "    president = []\n",
    "    agenda = ''\n",
    "    processed = []\n",
    "    \n",
    "    reg0 = re.compile(r\"^(adoption of the agenda|expression of.+|opening statement).*$\")\n",
    "    reg1 = re.compile(r\"^.+(\\.{1}|\\?|\\\")$\")\n",
    "    reg2 = re.compile(r\"^.+:.*$\")\n",
    "    reg3 = re.compile(r\"[\\t\\r\\n\\f]\")\n",
    "    reg4 = re.compile(r\"\\s\\s+\")\n",
    "    reg5 = re.compile(r\"^(\\d+|\\d+/\\d+.{0,1}|\\d{2}/\\d{2}/\\d{4}|\\d{2}-\\d{5}.*|S/PV\\.\\s*\\d{4})$\")\n",
    "    reg6 = re.compile(r\"^(\\d{4}\\D{2}\\smeeting|Security Council|(For|Fif|Six|Sev)\\D+\\syear|\\d+\\s\\S+\\s\\d{4})$\")\n",
    "    reg7 = re.compile(r\"^\\(((mr|mrs|ms|dr|sir|lord).+,.+|the president|the secretary-general|the secretary general)\\)$\")\n",
    "    reg8 = re.compile(r\"^(mr|mrs|ms|dr|sir|lord|the president|the secretary-general|the secretary general).*:.*$\")\n",
    "    reg9 = re.compile(r\"^(mr|mrs|ms|dr|sir|lord|the president|the secretary-general|the secretary general).*$\")\n",
    "    reg10 = re.compile(r\"^(arabic|russian|chinese|french|spanish|english).+$\")\n",
    "    reg11 = re.compile(r\"^\\*(\\s|\\*)+$\")\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        \n",
    "        line = str(line).strip()\n",
    "        \n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        \n",
    "        if not main:\n",
    "            \n",
    "            reg = re.compile(r\"^the meeting (was called to order|resumed|was resumed|was suspended).+$\")\n",
    "            if reg.match(line.strip('\\t').lower()):\n",
    "                main= True\n",
    "                continue\n",
    "            \n",
    "             if search_president:\n",
    "                reg = re.compile(r\"^(mr|mrs|ms|dr|sir|lord).+$\")\n",
    "                if reg.match(line.lower()):\n",
    "                    reg = re.compile(r\"\\.(\\s\\.)+\")\n",
    "                    pres_name = reg.sub('', line).replace('later', '').strip()\n",
    "                    president.append(pres_name)\n",
    "                    continue\n",
    "                reg = re.compile(r\"^\\(.+\\)$\")\n",
    "                if reg.match(line):\n",
    "                    pres_country = line.replace('(', '').replace(')', '').strip()\n",
    "                    president.append(pres_country)\n",
    "                    search_president = False\n",
    "                    continue\n",
    "            \n",
    "            if search_agenda[0]:\n",
    "                if line.lower() == 'agenda':\n",
    "                    search_agenda[1] = True\n",
    "                    continue\n",
    "                if search_agenda[1]:\n",
    "                    agenda = line\n",
    "                    if agenda.isupper():\n",
    "                        agenda = agenda.title()\n",
    "                    search_agenda[0] = False\n",
    "                    continue\n",
    "\n",
    "            continue\n",
    "        \n",
    "        reg = re.compile(r\"^the meeting (rose|was suspended).+$\")\n",
    "        if reg.match(line.lower()) or (i >= num_lines - 1):\n",
    "            break\n",
    "            \n",
    "        if form == 'pdf':\n",
    "            \n",
    "            if pdf_correction1[0]:\n",
    "                reg = re.compile(r\"^the president.*:\")\n",
    "                if reg0.match(line.lower()) or reg.match(line.lower()):\n",
    "                    pdf_correction1[0] = False\n",
    "                elif (len(line) > 0) and (not pdf_correction1[1]):\n",
    "                    pdf_correction1[1] = True\n",
    "            \n",
    "            if not pdf_correction2:\n",
    "                if line == 'The Security Council will now':\n",
    "                    pdf_correction2 = True\n",
    "                    line_to_add = len(processed)\n",
    "            else:\n",
    "                reg = re.compile(r\"^(begin|resume)$\")\n",
    "                if reg.match(line):\n",
    "                    processed[line_to_add] = (processed[line_to_add] + ' ' + line)\n",
    "                    continue\n",
    "                elif line == 'its':\n",
    "                    processed[line_to_add] = (processed[line_to_add] + ' ' + line)\n",
    "                    pdf_correction2 = False\n",
    "                    continue\n",
    "            \n",
    "            page_break = False\n",
    "            if (reg5.match(line)) or (reg6.match(line)) or (reg7.match(line.lower())):\n",
    "                page_break = True\n",
    "            elif (agenda_para > 1) and (line.lower() in agenda.lower()):\n",
    "                page_break = True\n",
    "            elif (agenda_para == 0) and ('resumption' in path.lower()) and (line.lower() in agenda.lower()):\n",
    "                page_break = True\n",
    "            if page_break:\n",
    "                if pdf_correction1[1] and (len(text_to_move) > 0):\n",
    "                    processed.extend(text_to_move)\n",
    "                    text_to_move = []\n",
    "                    pdf_correction1[1] = False\n",
    "                continue\n",
    "        \n",
    "        if search_agenda[0]:\n",
    "            if 'the agenda was adopted' in line.lower():\n",
    "                search_agenda[1] = True\n",
    "            elif search_agenda[1]:\n",
    "                agenda = line\n",
    "                if agenda.isupper():\n",
    "                    agenda = agenda.title()\n",
    "                search_agenda[0] = False\n",
    "        \n",
    "        if line[0].isupper() and reg0.match(line.lower()):\n",
    "            line = '--para--' + line + '--para--'\n",
    "          \n",
    "        else:\n",
    "            \n",
    "            if reg1.match(line):\n",
    "                line = line + '--para--'\n",
    "            \n",
    "            if reg2.match(line.strip('--para--')):\n",
    "                 if (line[0].isupper()) and (reg8.match(line.lower())):\n",
    "                    line = '--para--' + line\n",
    "                    if len(processed) > 0:\n",
    "                        previous = processed[-1]\n",
    "                        if not previous.endswith('\\n'):\n",
    "                            processed[-1] = previous + '\\n'\n",
    "                \n",
    "                elif len(processed) > 0:\n",
    "                    previous = processed[-1]\n",
    "                    if reg9.match(previous.lower()):\n",
    "                        processed[-1] = '\\n' + previous\n",
    "                        if len(processed) > 1:\n",
    "                            preceding = processed[-2]\n",
    "                            if not preceding.endswith('\\n'):\n",
    "                                processed[-2] = preceding + '\\n'\n",
    "        \n",
    "        if agenda_para == 0:\n",
    "            if 'adoption of the agenda' in line.lower():\n",
    "                agenda_para += 1\n",
    "        elif agenda_para == 1:\n",
    "            if reg8.match(line.lower().strip('--para--')):\n",
    "                agenda_para += 1\n",
    "            elif not line.endswith('--para--'):\n",
    "                if i + 1 < num_lines:\n",
    "                    next_line = lines[i+1].strip()\n",
    "                    if (len(next_line) > 0) and next_line[0].isupper():\n",
    "                        line = line + '--para--'\n",
    "                    elif len(next_line) == 0:\n",
    "                        line = line + '--para--'\n",
    "        \n",
    "        if reg11.match(line):\n",
    "            continue\n",
    "        \n",
    "        line = reg4.sub(' ', reg3.sub(' ', line))\n",
    "        line = line.strip().replace('--para--', '\\n')\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        \n",
    "        if (form == 'pdf') and pdf_correction1[0]:\n",
    "            text_to_move.append(line)\n",
    "            continue\n",
    "        \n",
    "        processed.append(line)\n",
    "    \n",
    "    text = ' '.join(processed).replace('\\n ', '\\n').replace(' \\n', '\\n').strip()\n",
    "    num_words = len(text.split())\n",
    "    num_paras = len(text.split('\\n\\n'))\n",
    "    return president, agenda, text, num_words, num_paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction of speech texts and other information from a single cleaned meeting record\n",
    "import re\n",
    "\n",
    "def speech_extraction(path, president_as_delegate=True, remove_quotes=True, speaker_thres=5):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    - path (string): path to an input text file that contains a cleaned speech record, i.e., output of clean_PV()\n",
    "    - president_as_delegate (boolean): if True, presidents' procedural statements are discarded\n",
    "    - remove_quotes (boolean): if True, all in-speech quotes are removed\n",
    "    - speaker_thres (integer): a threshold number of tokens for detecting a speaker's name\n",
    "    \n",
    "    Returns:\n",
    "    - records (2d list): each row consists of information on a single speech, which includes:\n",
    "      - speaker (string): the speaker's name\n",
    "      - country (string): the country represented by the speaker\n",
    "      - speech (string): speech text\n",
    "      - count (integer): word count of the speech\n",
    "    \"\"\"\n",
    "    \n",
    "    f = open(path, 'r')\n",
    "    text = f.read()\n",
    "    paras = text.split('\\n\\n')\n",
    "\n",
    "    speakers = []\n",
    "    countries = []\n",
    "    speeches = []\n",
    "    records = []\n",
    "\n",
    "    reg0 = re.compile(r\"^(mr|mrs|ms|dr|sir|lord|the president|the secretary-general|the secretary general).*:.*\")\n",
    "    reg1 = re.compile(r\"(in my capacity as|in my national capacity|resume my functions as)\")\n",
    "    reg2 = re.compile(r\"“[^“\\n]+”\")\n",
    "    reg3 = re.compile(r\"“[^”\\n]+\\n\")\n",
    "    reg4 = re.compile(r\"\\s—\\s\")\n",
    "    reg5 = re.compile(r\"\\s\\s+\")\n",
    "    reg6 = re.compile(r\"\\s\")\n",
    "    reg7 = re.compile(r\"\\\"[^\\\"\\n]+\\\"\")\n",
    "    reg8 = re.compile(r\"\\\"[^\\\"\\n]+\\n\")\n",
    "\n",
    "    for para in paras:\n",
    "        para = para.strip()\n",
    "        speaker = \"NULL\"\n",
    "        country = \"NULL\"\n",
    "        speech = \"NULL\"\n",
    "        if reg0.match(para.lower()):\n",
    "            parts = para.split(':')\n",
    "            head = parts[0]\n",
    "            speech = (':'.join(parts[1:])).strip()\n",
    "\n",
    "            if remove_quotes:\n",
    "                speech = reg5.sub(' ', reg4.sub(' ', reg3.sub('', reg2.sub('', speech))))\n",
    "                speech = reg5.sub(' ', reg4.sub(' ', reg8.sub('', reg7.sub('', speech))))\n",
    "    \n",
    "            part = head.split('(')[0].strip()\n",
    "            if len(part.split()) <= speaker_thres:\n",
    "                speaker = part.strip()\n",
    "                country = 'n.a.'\n",
    "                if ('(' in head) and (')' in head):\n",
    "                    country = head.split('(')[1].split(')')[0].strip()\n",
    "            else:\n",
    "                if len(speeches) > 0:\n",
    "                    previous = speeches[-1]\n",
    "                    if previous != \"NULL\":\n",
    "                        speeches[-1] = (previous + ' ' + para)\n",
    "                continue\n",
    "                \n",
    "            if (speaker.lower() == 'mr. president') or (speaker.lower() == 'ms. president'):\n",
    "                speaker = 'The President'\n",
    "            \n",
    "            if speaker.lower() == 'the president' and president_as_delegate:\n",
    "                if not reg1.search(reg5.sub(' ', speech.replace('\\n', ' ').lower())):\n",
    "                    speaker = \"NULL\"\n",
    "                    country = \"NULL\"\n",
    "                    speech = \"NULL\"\n",
    "        \n",
    "        speakers.append(speaker)\n",
    "        countries.append(country)\n",
    "        speeches.append(speech)\n",
    "\n",
    "    num_speakers = len(speakers)\n",
    "    for i in range(num_speakers):\n",
    "        speaker = speakers[i].title()\n",
    "        country = countries[i]\n",
    "        if speakers[i].upper() == \"NULL\":\n",
    "            continue\n",
    "        if speaker.lower() == 'the president':\n",
    "            country = 'n.a.'\n",
    "        elif (country.lower().startswith('spoke')) or (country.lower().startswith('interpretation')):\n",
    "            country = 'n.a.'\n",
    "        speech = speeches[i]\n",
    "        count = len(reg5.sub(' ', reg6.sub(' ', speech)).split())\n",
    "        if count == 0:\n",
    "            continue\n",
    "        row = [speaker, country, speech, count]\n",
    "        records.append(row)\n",
    "    \n",
    "    return records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procedures for text preprocessing\n",
    "Input  \n",
    "speeches (list): a list of extracted speeches  \n",
    "Output  \n",
    "corpus_final (list): a list of bags of preprocessed POS-tagged tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part-of-speech (POS) tagging\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "\n",
    "tagged_speeches = []\n",
    "for i, speech in enumerate(speeches):\n",
    "    if i % 1000 == 0:\n",
    "        print('processing doc %i...' % i)\n",
    "    tagged = nltk.pos_tag(tokenize.word_tokenize(speech))\n",
    "    tagged_speeches.append(tagged)\n",
    "print('%i docs have been tagged.' % len(tagged_speeches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a standard menu for text preprocessing: \n",
    "# bag-of-words conversion, lower-case conversion, stopword removal...\n",
    "import re\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stops = stopwords.words('english')\n",
    "reg = re.compile(r'[^a-zA-Z_0-9\\-]')\n",
    "\n",
    "tagged_corpus = []\n",
    "for i, tagged in enumerate(tagged_speeches):\n",
    "    if i % 5000 == 0:\n",
    "        print(\"processing doc %i...\" % i)\n",
    "    processed = []\n",
    "    for token, pos in tagged:\n",
    "        token = reg.sub('', token.lower())\n",
    "        if (len(token) > 0) & (token not in stops):\n",
    "            processed.append((token, pos))\n",
    "    tagged_corpus.append(processed)\n",
    "print(\"%i docs have been processed.\" % len(tagged_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spelling conversion from British to American spelling\n",
    "\n",
    "# file paths of the spelling lists\n",
    "path1 = 'supplements/british.txt'\n",
    "path2 = 'supplements/american.txt'\n",
    "\n",
    "with open(path1, 'r') as f:\n",
    "    british = f.read().replace(' ', '').split('\\n')\n",
    "with open(path2, 'r') as f:\n",
    "    american = f.read().replace(' ', '').split('\\n')\n",
    "spell_dict = dict(zip(british, american))\n",
    "\n",
    "tagged_corpus2 = []\n",
    "count = 0\n",
    "for i, tagged in enumerate(tagged_corpus):\n",
    "    if i % 5000 == 0:\n",
    "        print(\"processing doc %i...\" % i)\n",
    "    processed = []\n",
    "    for token, pos in tagged:\n",
    "        if token in british:\n",
    "            processed.append((spell_dict[token], pos))\n",
    "            count += 1\n",
    "        else:\n",
    "            processed.append((token, pos))\n",
    "    tagged_corpus2.append(processed)\n",
    "print(\"%i docs have been processed.\" % len(tagged_corpus2))\n",
    "print(\"%i words have been converted.\" % count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining each word and its POS tag with a colon\n",
    "tagged_corpus3 = []\n",
    "for i, tagged in enumerate(tagged_corpus2):\n",
    "    if i % 5000 == 0:\n",
    "        print(\"processing doc %i...\" % i)\n",
    "    processed = []\n",
    "    for word, pos in tagged:\n",
    "        token = ':'.join([word, pos])\n",
    "        processed.append(token)\n",
    "    doc = ' '.join(processed)\n",
    "    tagged_corpus3.append(doc)\n",
    "print(\"%i docs have been processed.\" % len(tagged_corpus3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting phrase detection...\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "corpus_uni = []\n",
    "for line in tagged_corpus3:\n",
    "    corpus_uni.append(line.strip().split(\" \"))\n",
    "print(\"%i docs have been converted.\" % len(corpus_uni))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First iteration: generating bigrams\n",
    "min_count = 1000\n",
    "thres = 5.0\n",
    "\n",
    "phrases_bi = Phrases(corpus_uni, min_count=min_count, threshold=thres)\n",
    "bigram = Phraser(phrases_bi)\n",
    "transformed_bi = bigram[corpus_uni]\n",
    "\n",
    "corpus_bi = []\n",
    "for bi in transformed_bi:\n",
    "    corpus_bi.append(bi)\n",
    "len(corpus_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second iteration: generating trigrams\n",
    "min_count = 1000\n",
    "thres = 5.0\n",
    "\n",
    "phrases_tri = Phrases(corpus_bi, min_count=min_count, threshold=thres)\n",
    "trigram = Phraser(phrases_tri)\n",
    "transformed_tri = trigram[corpus_bi]\n",
    "\n",
    "corpus_final = []\n",
    "for tri in transformed_tri:\n",
    "    corpus_final.append(tri)\n",
    "len(corpus_final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
