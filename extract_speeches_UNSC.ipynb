{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7saVVf4AdTXZ"
      },
      "source": [
        "## Cleaning and Extraction of Speech Transcripts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9vMzoH2dTXb"
      },
      "source": [
        "### Text Extraction from Word Documents OCR’d with ABBYY FineReader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ioSkHJcudTXc",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Define the parent folder containing the original Word files\n",
        "parent_folder = r\"PATH_TO_ORIGINAL_WORD_FILES\"  # Replace with your folder path containing Word files\n",
        "\n",
        "# Define the output folder for saving the extracted text files\n",
        "output_folder = r\"PATH_TO_EXTRACTED_TEXT\"  # Replace with your folder path for saving processed text files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSkkRSc3dTXc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from docx import Document\n",
        "from langdetect import detect\n",
        "\n",
        "# Ensure the output folder exists\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "# Function to check if a paragraph is in English using langdetect\n",
        "def is_english_langdetect(paragraph_text):\n",
        "    try:\n",
        "        return detect(paragraph_text) == 'en'\n",
        "    except Exception:\n",
        "        return False  # If detection fails, assume the text is not English\n",
        "\n",
        "# Iterate over all Word files in the parent folder\n",
        "for file_name in os.listdir(parent_folder):\n",
        "    if file_name.endswith('.docx'):\n",
        "        # Load the document\n",
        "        file_path = os.path.join(parent_folder, file_name)\n",
        "        doc = Document(file_path)\n",
        "\n",
        "        # Initialize text variable for processed content\n",
        "        processed_text_langdetect = \"\"\n",
        "\n",
        "        # Process each paragraph in the document\n",
        "        for para in doc.paragraphs:\n",
        "            if para.text.strip() and is_english_langdetect(para.text):\n",
        "                # Remove hyphenation and newlines within paragraphs\n",
        "                processed_paragraph = para.text.strip().replace('-\\n', '').replace('\\n', ' ')\n",
        "                processed_text_langdetect += processed_paragraph + \"\\n\\n\"\n",
        "\n",
        "        # Save the processed text to a new text file\n",
        "        output_file_name = os.path.splitext(file_name)[0] + \".txt\"\n",
        "        output_file_path = os.path.join(output_folder, output_file_name)\n",
        "\n",
        "        with open(output_file_path, \"w\", encoding=\"utf-8\") as text_file:\n",
        "            text_file.write(processed_text_langdetect.strip())\n",
        "\n",
        "        print(f\"Processed and saved: {output_file_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHDjxtKAdTXd"
      },
      "source": [
        "### Cleaning and Parsing the Extracted Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1M0uALB_dTXd"
      },
      "outputs": [],
      "source": [
        "# Get file paths for the target text files\n",
        "import glob\n",
        "\n",
        "# Change the following directory to your temporary text files folder\n",
        "# in_dir = \"PATH_TO_TEMP_FILES/\"\n",
        "in_dir = \"PATH_TO_TEMP_FILES/\"\n",
        "files = glob.glob(in_dir + '*.txt')\n",
        "num_files = len(files)\n",
        "print(\"{} files\".format(num_files))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ua-5A89OdTXe"
      },
      "outputs": [],
      "source": [
        "# Load the correction mapping table\n",
        "import pandas as pd\n",
        "\n",
        "# Replace with your corrections file path\n",
        "corr_df = pd.read_csv(\"PATH_TO_CORRECTIONS_FILE/corrections.csv\")\n",
        "corr_df.dropna(inplace=True)\n",
        "corr_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcFqHYdwdTXe"
      },
      "outputs": [],
      "source": [
        "# Create a dictionary for the correction mappings\n",
        "before = list(corr_df.before)\n",
        "after = list(corr_df.after)\n",
        "corr_list = [(str(before[i]).strip(), str(after[i]).strip()) for i in range(len(before))]\n",
        "corr_dict = dict(corr_list)\n",
        "keys = corr_dict.keys()\n",
        "len(corr_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9__xVTGdTXf"
      },
      "outputs": [],
      "source": [
        "corr_dict[\"The PRESIDENT;\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-y7CivTdTXf"
      },
      "outputs": [],
      "source": [
        "# Load the titles (honorific and position labels) table\n",
        "# Replace with your titles file path\n",
        "titles_df = pd.read_csv(\"PATH_TO_TITLES_FILE/titles.csv\", dtype=str)\n",
        "titles_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-s1e2eSydTXg"
      },
      "outputs": [],
      "source": [
        "# Build lists for each category\n",
        "position_list = list(titles_df[titles_df[\"type\"] == \"position\"][\"title\"])\n",
        "title_pre_list = list(titles_df[titles_df[\"type\"] == \"pre\"][\"title\"])\n",
        "title_post_list = list(titles_df[titles_df[\"type\"] == \"not_pre\"][\"title\"])\n",
        "print(len(position_list), len(title_pre_list), len(title_post_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5chHlH_tdTXg"
      },
      "outputs": [],
      "source": [
        "# Check the contents\n",
        "title_pre_list[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1HrfKSDdTXg"
      },
      "outputs": [],
      "source": [
        "# Build regular expressions including honorific/position labels\n",
        "positions_or = \"|\".join([str(pos).strip() for pos in position_list])\n",
        "pre_or = \"|\".join([str(title).strip() for title in title_pre_list])\n",
        "post_or1 = \"|\".join([str(title).strip() for title in title_post_list])\n",
        "post_or2 = \"|\".join([r\"[^\\(\\):]+\\s\" + str(title).strip() for title in title_post_list])\n",
        "titles_all_or = \"|\".join([pre_or, post_or2, positions_or])\n",
        "titles_all_or"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nk1Pi72MdTXh"
      },
      "outputs": [],
      "source": [
        "# Clean and format the meeting transcript text files\n",
        "import re\n",
        "from langdetect import detect\n",
        "\n",
        "def clean_PV(path):\n",
        "\n",
        "    # Compile regular expressions for identifying document start and end\n",
        "    reg_start = re.compile(r\"^(the meeting (was called to order|resumed|was resumed|was suspended)|held (at|in)).+$\", re.IGNORECASE)\n",
        "    reg_start_lenient = re.compile(r\"^(the meeting was called to order|held (at|in)).+$\", re.IGNORECASE)\n",
        "    reg_agenda = re.compile(r\"^the agenda was (adopted|that).+$\", re.IGNORECASE)\n",
        "    reg_end = re.compile(r\"^(the meeting (rose|was suspended)|the council (rose|was suspended)).+$\", re.IGNORECASE)\n",
        "    reg_end_lenient = re.compile(r\"^(the meeting rose|the council rose).+$\", re.IGNORECASE)\n",
        "\n",
        "    # Regular expressions to identify headers and footers\n",
        "    reg_head1 = re.compile(r\"^(\\d+|\\d+-\\d+|\\d+/\\d+.{0,1}|\\d{2}/\\d{2}/\\d{4}|\\d{2}-\\d{5}.*|S/PV\\.\\s*\\d{4})$\")\n",
        "    reg_head2 = re.compile(r\"^(\\d{2}/\\d{2}/\\d{4}\\s+\\S+\\s+S/PV\\.\\s*\\d{4}|S/PV\\.\\s*\\d{4}\\s+\\S+\\s+\\d{2}/\\d{2}/\\d{4})$\")\n",
        "    reg_head3 = re.compile(r\"^(\\d{4}\\D{2}\\smeeting|Security Council|(Twe|Thi|For|Fif|Six|Sev)\\D+\\syear|\\d+\\s\\S+\\s\\d{4})$\")\n",
        "    reg_head4 = re.compile(\"^(\\(|\\{)((\"+pre_or+\").+?(\\,.+)*?|\"+\"(\"+post_or2+\")\\s*?(\\,.+)*?|\"+positions_or+\")(\\)|\\})$\", re.IGNORECASE)\n",
        "\n",
        "    # Regular expression to identify footnotes\n",
        "    reg_footnote = re.compile(r\"^(\\d+\\s*/\\s+.*$|(\\d+?\\/\\s+|)(ibid\\.|op\\.cit\\.|idem|notes see\\s+?|see\\s+.*?official records of the (general assembly|security council)|see\\s+.*?(general assembly|security council) resolution|official records of the (general assembly|security council)|for the text of the report see).*$)\", re.IGNORECASE)\n",
        "\n",
        "    # Matching for paragraph numbering and punctuation at the beginning\n",
        "    reg_paranum = re.compile(r\"^(|\\d|\\d\\s*?\\d|\\d\\s*?\\d\\s*?\\d)\\s*?(\\.|\\,|:|;|\\*\\.|\\.\\*|\\^\\.|■)\\s*(.+)$\")\n",
        "    # Corrections for misrecognized articles\n",
        "    reg_article1 = re.compile(r\"(^|\\s+)(t|T)(he|ho|te|h®|h©|ha|h\\*|he •)\\s+\")\n",
        "    reg_article2 = re.compile(r\"(^|\\s+)(z|Z)he\\s+\")\n",
        "    # Corrections for misrecognized 'president'\n",
        "    reg_president = re.compile(r\"^(the president|the presidemt|tho president|tte president|th® prkideht|th© president|tha president|th\\* president|zhe president|the presment|the président|the • president)\", re.IGNORECASE)\n",
        "    reg_president_inclusive = re.compile(r\"(^|\\s+)(the|The)\\s+?(p|P)(resident|residemt|rkideht|residet|resment|résident|RESIDENT|RESIDEMT|RKIDEHT|RESIDET|RESMENT|RÉSIDENT)\")\n",
        "    # Corrections for misrecognized honorifics\n",
        "    reg_mr = re.compile(r\"^(Me|Hr|Mf|Uh|Nr)\\.\\s+\")\n",
        "    reg_ms = re.compile(r\"^(Ma)\\.\\s+\")\n",
        "    reg_title = re.compile(r\"^(mr|mrs|ms|dr)(|\\,|;|:|»|\\s+?\\.)(\\s+)\", re.IGNORECASE)\n",
        "    reg_title_inclusive = re.compile(r\"(^|\\s+)(Mr|Mrs|Ms|Dr)(|\\,|;|:|»|\\s+?\\.)\\s+\")\n",
        "    reg_title_nospace = re.compile(r\"(^|\\s+)(Mr|Mrs|Ms|Dr)(|\\.|\\,|;|:|»)([A-Z])\")\n",
        "    # Correction for double parentheses misrecognition\n",
        "    reg_doubleparenth = re.compile(\"^(\"+titles_all_or+\")([^\\(\\):]*?\\([^\\(\\):]+?\\))\\S\\s*?(\\([^\\(\\):]+?\\))\\s*?:\\s+\", re.IGNORECASE)\n",
        "    # Exclude cases where a colon is misrecognized (e.g., listing attendees)\n",
        "    reg_present = re.compile(\"^((\"+pre_or+\")[^\\(\\):]+?\\([^\\(\\):]+?\\)\\,\\s*?){2,}?\", re.IGNORECASE)\n",
        "    # Corrections for colon misrecognition: cases following parentheses and in chairman’s statements\n",
        "    reg_colon1 = re.compile(\"^(\"+titles_all_or+\")([^\\(\\):]*?\\([^\\(\\):]+?\\))\\s*?(\\([^\\(\\):]+?\\))*?\\s*?(:|;|\\,|\\.|’\\.|'\\.|-\\.|!|\\?|i|r|t|s|\\*\\.|\\.\\*|\\*|»|\\,»|\\\"\\,|\\}\\.|\\.•)\\s+\", re.IGNORECASE)\n",
        "    reg_colon2 = re.compile(\"^(\"+positions_or+\")\\s*?(:|;|’\\.|'\\.|-\\.|!|\\?|r|t|s|\\*\\.|\\.\\*|\\*|»|\\,»|\\\"\\,|\\}\\.|\\.•)\\s+\", re.IGNORECASE)\n",
        "    reg_colon3 = re.compile(\"^The (President|PRESIDENT)(|\\s*?(i|\\.|\\,))\\s+(I|Members of the Council have before them|In accordance with|The Security Council will now|The first speaker is|The next speaker is|There are no further speakers|There are no more speakers|There were \\d+ votes)\\s+\")\n",
        "    # Matching for in-sentence speaker changes\n",
        "    reg_insent1 = re.compile(\"(.+)(\"+titles_all_or+\")([^\\(\\):]*?\\([^\\(\\):]+?\\))\\s*?(\\([^\\(\\):]+?\\))*?\\s*?:\\s+\", re.IGNORECASE)\n",
        "    reg_insent2 = re.compile(\"(.+)(\"+positions_or+\")\\s*?:\\s+\", re.IGNORECASE)\n",
        "\n",
        "    # Correction for misrecognized 'I' as 'X/x'\n",
        "    reg_ix = re.compile(r\"(^|\\s+)(X|x)(|t|ts|n|f)\\s+?(\\w)\")\n",
        "    # Matching for headers containing speaker info at the beginning\n",
        "    reg_headin = re.compile(r\"(^|\\s+)\\(((\"+pre_or+\")[^\\(\\)]+?(\\,[^\\(\\)]+)*?|\"+\"(\"+post_or2+\")\\s*?(\\,[^\\(\\)]+)*?|\"+positions_or+\")\\)\\s+\", re.IGNORECASE)\n",
        "    # Matching for extra symbols\n",
        "    reg_remove = re.compile(r\"^\\*(\\s|\\*)+$\")\n",
        "    # Matching for hyphenation within words\n",
        "    reg_hyphen = re.compile(r\"^(.+)\\w\\-$\")\n",
        "\n",
        "    # Open the file and start processing\n",
        "    with open(path, 'r') as f:\n",
        "        txt = f.read()\n",
        "\n",
        "        # First apply the correction mapping to the entire text\n",
        "        for key in keys:\n",
        "            txt = txt.replace(key, corr_dict[key])\n",
        "\n",
        "        # Process the text line by line\n",
        "        lines = txt.split(\"\\n\")\n",
        "        num_lines = len(lines)\n",
        "        main = False\n",
        "        processed = []\n",
        "        processed2 = []\n",
        "\n",
        "        for i, line in enumerate(lines):\n",
        "            line = str(line).strip()\n",
        "\n",
        "            # Skip empty lines\n",
        "            if len(line) == 0:\n",
        "                continue\n",
        "\n",
        "            # Correct typical misrecognitions\n",
        "            line = line.replace(\"{\", \"(\").replace(\"[\", \"(\").replace(\"}\", \")\").replace(\"]\", \")\")\n",
        "            line = reg_paranum.sub(r'\\3', line).strip()\n",
        "            line = reg_ix.sub(r\"\\1I\\3 \\4\", line)\n",
        "            line = reg_article1.sub(r\"\\1\\2he \", line)\n",
        "            if reg_article2.match(line):\n",
        "                if line[0].isupper():\n",
        "                    line = reg_article2.sub(r\"\\1The \", line)\n",
        "                else:\n",
        "                    line = reg_article2.sub(r\"\\1the \", line)\n",
        "            line = reg_president_inclusive.sub(r\"\\1\\2 \\3resident\", line)\n",
        "            line = reg_mr.sub(r\"Mr. \", line)\n",
        "            line = reg_ms.sub(r\"Ms. \", line)\n",
        "            line = reg_title_inclusive.sub(r\"\\1\\2. \", line)\n",
        "            line = reg_title_nospace.sub(r\"\\1\\2. \\4\", line)\n",
        "            reg_doubleparenth.sub(r\"\\1\\2\\3: \", line)\n",
        "            if not reg_present.match(line):\n",
        "                line = reg_colon1.sub(r\"\\1\\2\\3: \", line)\n",
        "            line = reg_colon2.sub(r\"\\1: \", line)\n",
        "            line = reg_colon3.sub(r\"The \\1: \\4 \", line)\n",
        "            line = line.replace(\"::\", \":\")\n",
        "#             line = reg_insent1.sub(r\"\\1\\n\\n\\2\\3\\4: \", line)\n",
        "#             line = reg_insent2.sub(r\"\\1\\n\\n\\2: \", line)\n",
        "            if reg_headin.match(line):\n",
        "                line = reg_headin.sub(\"\", line)\n",
        "\n",
        "            # If the line starts with a lowercase letter, attach it to the previous paragraph\n",
        "            if (i > 0) and (line[0].islower()):\n",
        "                if main:\n",
        "                    if len(processed) > 0:\n",
        "                        preceding = processed[-1]\n",
        "                        if reg_hyphen.match(preceding):\n",
        "                            line = (preceding.replace(\"-\", \"\") + line)\n",
        "                        else:\n",
        "                            line = (preceding + \" \" + line)\n",
        "                        processed[-1] = line\n",
        "                        continue\n",
        "                else:\n",
        "                    if len(processed2) > 0:\n",
        "                        preceding = processed2[-1]\n",
        "                        if reg_hyphen.match(preceding):\n",
        "                            line = (preceding.replace(\"-\", \"\") + line)\n",
        "                        else:\n",
        "                            line = (preceding + \" \" + line)\n",
        "                        processed2[-1] = line\n",
        "                        continue\n",
        "\n",
        "            if len(line) == 0:\n",
        "                continue\n",
        "            if (reg_head1.match(line)) or (reg_head2.match(line)) or (reg_head3.match(line)) or (reg_head4.match(line)):\n",
        "                continue\n",
        "            if reg_footnote.match(line):\n",
        "                continue\n",
        "            if reg_remove.match(line):\n",
        "                continue\n",
        "\n",
        "#             try:\n",
        "#                 if detect(line) != 'en':\n",
        "#                     continue\n",
        "#             except:\n",
        "#                 continue\n",
        "\n",
        "            # Determine if we are in the main body of the document\n",
        "            if not main:\n",
        "                if reg_start_lenient.match(line):\n",
        "                    main = True\n",
        "#                 if reg_agenda.match(line):\n",
        "#                     main = True\n",
        "\n",
        "            if main:\n",
        "                processed.append(line.strip())\n",
        "            else:\n",
        "                processed2.append(line.strip())\n",
        "\n",
        "            if main and reg_end_lenient.match(line):\n",
        "                break\n",
        "\n",
        "        if main:\n",
        "            text = '\\n\\n'.join(processed).strip()\n",
        "        else:\n",
        "            text = '\\n\\n'.join(processed2).strip()\n",
        "\n",
        "        num_words = len(text.split())\n",
        "        num_paras = len(text.split('\\n\\n'))\n",
        "\n",
        "    return text, num_words, num_paras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOywm_z2dTXh"
      },
      "outputs": [],
      "source": [
        "# Process and clean the target files\n",
        "import os\n",
        "\n",
        "# Set the output directory for cleaned texts\n",
        "out_dir = r\"PATH_TO_CLEANED_TEXT\"  # Replace with your folder path for cleaned text files\n",
        "# Tag to append to the cleaned file names\n",
        "tag_out = \"_clean\"\n",
        "\n",
        "# Regular expression for extracting meeting numbers from file names\n",
        "reg_meetingnum = re.compile(r\"S_PV\\.(\\d+).*?\\.txt\")\n",
        "\n",
        "# List to record meeting number, document name, word count, paragraph count, and file path\n",
        "results = []\n",
        "for i, f_path in enumerate(files):\n",
        "    if i % 100 == 0:\n",
        "        print(\"Processing document {}/{}\".format(i+1, num_files))\n",
        "    meeting_num = int(reg_meetingnum.sub(r\"\\1\", os.path.basename(f_path)))\n",
        "    file_name = os.path.splitext(os.path.basename(f_path))[0]\n",
        "    text, num_words, num_paras = clean_PV(f_path)\n",
        "    out_file = file_name + tag_out + \".txt\"\n",
        "    out_path = os.path.join(out_dir, out_file)\n",
        "    with open(out_path, \"w\") as f:\n",
        "        f.write(text)\n",
        "    results.append([meeting_num, file_name, num_words, num_paras, out_path])\n",
        "print(\"{} files have been processed.\".format(len(results)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcBeg7DadTXh"
      },
      "outputs": [],
      "source": [
        "# Save the cleaned document list as a DataFrame\n",
        "cols = [\"meeting_num\", \"doc_name\", \"word_count\", \"para_count\", \"path\"]\n",
        "df_cleaned = pd.DataFrame(results, columns=cols).sort_values([\"meeting_num\", \"doc_name\"]).reset_index(drop=True)\n",
        "df_cleaned.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYbqE9ofdTXh"
      },
      "outputs": [],
      "source": [
        "df_cleaned.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wecQnt0sdTXh"
      },
      "outputs": [],
      "source": [
        "# Save the document list\n",
        "out_path = r\"PATH_TO_OUTPUT_DF/UNSC_PV_cleaned.csv\"  # Replace with your output path\n",
        "df_cleaned.to_csv(out_path, na_rep='NULL')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oaZG9QTdTXi"
      },
      "source": [
        "### Merging Meeting Data with the Cleaned Document List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIAyodoJdTXi"
      },
      "outputs": [],
      "source": [
        "# Load the cleaned document list DataFrame\n",
        "import pandas as pd\n",
        "\n",
        "# Replace with your cleaned document list file path\n",
        "in_path = r\"PATH_TO_OUTPUT_DF/UNSC_PV_cleaned.csv\"\n",
        "df_cleaned = pd.read_csv(in_path, na_values='NULL', index_col=0)\n",
        "df_cleaned.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83744pfVdTXi"
      },
      "outputs": [],
      "source": [
        "# Load the meetings data\n",
        "# Replace with your meetings file path\n",
        "in_path = r\"PATH_TO_MEETINGS_FILE/meetings.tsv\"\n",
        "df_meetings = pd.read_csv(in_path, na_values='NULL', index_col=0, sep='\\t')\n",
        "df_meetings.tail(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FA8SeJUhdTXi"
      },
      "outputs": [],
      "source": [
        "# Merge the document list with the meeting data to augment with meeting details\n",
        "# Also update the word count in the meeting data\n",
        "import numpy as np\n",
        "\n",
        "# List of documents to skip\n",
        "to_skip = [\"S_PV.2977(PartI)-EN\", \"S_PV.2977(PartII)(closed)-EN\", \"S_PV.2977(PartII)(closed-resumption1)-EN\",\n",
        "           \"S_PV.2977(PartII)(closed-resumption2)-EN\", \"S_PV.2977(PartII)(closed-resumption3)-EN\",\n",
        "           \"S_PV.2977(PartII)(closed-resumption4)-EN\", \"S_PV.2977(PartII)(closed-resumption5)-EN\", \"S_PV.3160_RU\"]\n",
        "\n",
        "num_docs = df_cleaned.shape[0]\n",
        "results = []\n",
        "# For each document, match and extract the meeting info\n",
        "for i in range(num_docs):\n",
        "    if i % 100 == 0:\n",
        "        print(\"Processing document {}/{}\".format(i+1, num_docs))\n",
        "    series = df_cleaned.iloc[i]\n",
        "    meeting_num = series[\"meeting_num\"]\n",
        "    doc_name = series[\"doc_name\"]\n",
        "    if doc_name in to_skip:\n",
        "        results.append([\"NULL\", -1, -1, -1, \"NULL\", \"NULL\", \"NULL\", \"NULL\"])\n",
        "        continue\n",
        "    select = (df_meetings[\"meeting_num\"] == meeting_num)\n",
        "    if select.sum() > 1:\n",
        "        print(\"multiple matches\", doc_name)\n",
        "        df_meetings_select = df_meetings[select]\n",
        "        select2 = (df_meetings_select[\"record\"] == doc_name.replace(\"_\", \"/\"))\n",
        "        if select2.sum() == 0:\n",
        "            print(\"in the end, no match\", doc_name)\n",
        "            results.append([\"NULL\", -1, -1, -1, \"NULL\", \"NULL\", \"NULL\", \"NULL\"])\n",
        "        else:\n",
        "            match = df_meetings_select[select2].iloc[0]\n",
        "            record_id = match[\"record_id\"]\n",
        "            year = match[\"year\"]\n",
        "            month = match[\"month\"]\n",
        "            day = match[\"day\"]\n",
        "            topic = match[\"topic\"]\n",
        "            agenda = match[\"agenda\"]\n",
        "            pres_name = match[\"pres_name\"]\n",
        "            pres_country = match[\"pres_country\"]\n",
        "            results.append([record_id, year, month, day, topic, agenda, pres_name, pres_country])\n",
        "            df_meetings.loc[match.name, \"word_count\"] = int(series[\"word_count\"])\n",
        "    elif select.sum() == 0:\n",
        "        print(\"no match\", doc_name)\n",
        "        results.append([\"NULL\", -1, -1, -1, \"NULL\", \"NULL\", \"NULL\", \"NULL\"])\n",
        "    else:\n",
        "        match = df_meetings[select].iloc[0]\n",
        "        record_id = match[\"record_id\"]\n",
        "        year = match[\"year\"]\n",
        "        month = match[\"month\"]\n",
        "        day = match[\"day\"]\n",
        "        topic = match[\"topic\"]\n",
        "        agenda = match[\"agenda\"]\n",
        "        pres_name = match[\"pres_name\"]\n",
        "        pres_country = match[\"pres_country\"]\n",
        "        results.append([record_id, year, month, day, topic, agenda, pres_name, pres_country])\n",
        "        df_meetings.loc[match.name, \"word_count\"] = int(series[\"word_count\"])\n",
        "print(\"{} documents have been processed.\".format(len(results)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZDvGxK6dTXi"
      },
      "outputs": [],
      "source": [
        "# Combine the extracted meeting data with the document list\n",
        "cols = [\"record_id\", \"year\", \"month\", \"day\", \"topic\", \"agenda\", \"pres_name\", \"pres_country\"]\n",
        "df_to_add = pd.DataFrame(results, columns=cols)\n",
        "df_combined = pd.concat([df_cleaned, df_to_add], axis=1)\n",
        "df_combined.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZCMgdhodTXj"
      },
      "outputs": [],
      "source": [
        "df_combined.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwCoaPgxdTXj"
      },
      "outputs": [],
      "source": [
        "# Save the augmented document list\n",
        "out_path = r\"PATH_TO_OUTPUT_DF/UNSC_PV_cleaned.csv\"  # Replace with your output path\n",
        "df_combined.to_csv(out_path, na_rep='NULL')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAS-JKTMdTXj"
      },
      "outputs": [],
      "source": [
        "df_combined.tail(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tw2EmFedTXj"
      },
      "outputs": [],
      "source": [
        "# Check the updated meetings data\n",
        "df_meetings.tail(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFm2KTOZdTXj"
      },
      "outputs": [],
      "source": [
        "df_meetings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xw0hLVKkdTXj"
      },
      "outputs": [],
      "source": [
        "# Save the updated meetings data\n",
        "out_path = r\"PATH_TO_MEETINGS_FILE/meetings.tsv\"  # Replace with your meetings output path\n",
        "df_meetings.to_csv(out_path, sep='\\t', na_rep='NULL')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI35DmcHdTXm"
      },
      "source": [
        "### Extracting Speaker and Speech Data from Cleaned Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qlbD4asdTXm"
      },
      "outputs": [],
      "source": [
        "# Load the cleaned document list\n",
        "import pandas as pd\n",
        "\n",
        "# Replace with your cleaned document list file path\n",
        "in_path = r\"PATH_TO_OUTPUT_DF/UNSC_PV_cleaned.csv\"\n",
        "df_cleaned = pd.read_csv(in_path, na_values='NULL', index_col=0)\n",
        "df_cleaned.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVg4KyMRdTXm"
      },
      "outputs": [],
      "source": [
        "df_cleaned.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBFebVX0dTXm"
      },
      "outputs": [],
      "source": [
        "# Remove rows with no matching meeting data\n",
        "df_cleaned.dropna(subset=[\"record_id\"], inplace=True)\n",
        "df_cleaned.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EghyWdXAdTXn"
      },
      "outputs": [],
      "source": [
        "# Load the titles table\n",
        "titles_df = pd.read_csv(\"PATH_TO_TITLES_FILE/titles.csv\", dtype=str)\n",
        "titles_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SolN5wnRdTXn"
      },
      "outputs": [],
      "source": [
        "# Build lists for each category\n",
        "position_list = list(titles_df[titles_df[\"type\"] == \"position\"][\"title\"])\n",
        "title_pre_list = list(titles_df[titles_df[\"type\"] == \"pre\"][\"title\"])\n",
        "title_post_list = list(titles_df[titles_df[\"type\"] == \"not_pre\"][\"title\"])\n",
        "print(len(position_list), len(title_pre_list), len(title_post_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3Aw1lb1dTXn"
      },
      "outputs": [],
      "source": [
        "# Build regular expressions for honorific/position labels\n",
        "positions_or = \"|\".join([str(pos).strip() for pos in position_list])\n",
        "pre_or = \"|\".join([str(title).strip() for title in title_pre_list])\n",
        "post_or1 = \"|\".join([str(title).strip() for title in title_post_list])\n",
        "post_or2 = \"|\".join([r\"[^\\(\\):]+\\s\" + str(title).strip() for title in title_post_list])\n",
        "positions_or"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHpABR-bdTXn"
      },
      "outputs": [],
      "source": [
        "# Function to determine if a string is a name (only surname)\n",
        "import re\n",
        "\n",
        "def is_nameonly(string, name_thres=5):\n",
        "    reg_particles1 = re.compile(r\"\\s+(of|the|de|la|le|du|des|les|von|zu|auf|und|van|der|do|dos|da|das|e|del|el|of that Ilk|di|dei|de\\’|de\\'|della|dal|dalla|dai|tot|thoe|af|aw|ag|al|à|den|wa)\\b\")\n",
        "    reg_particles2 = re.compile(r\"\\s+(d\\'|d\\’|el\\-|al\\-)\")\n",
        "    reg_space = re.compile(r\"\\s+\")\n",
        "    string = reg_space.sub(r\" \", reg_particles1.sub(r\" \", reg_particles2.sub(r\" \", string)))\n",
        "    parts = string.split()\n",
        "    if len(parts) > name_thres:\n",
        "        return False\n",
        "    for part in parts:\n",
        "        if part[0].islower():\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "# Function to extract speaker and speech information from a transcript\n",
        "def speech_extraction(path, speaker_thres=5, remove_parentheses=True, remove_quotes=True):\n",
        "    import re\n",
        "\n",
        "    # Regular expressions for different speaker patterns\n",
        "    reg_speaker1 = re.compile(\"^(\"+pre_or+\")(\\s+?\\w+?[^\\(\\):]*?)(\\([^\\(\\):]+?\\))??\\s*?(\\([^\\(\\):]+?\\))*?\\s*?:\\s*?(.+)$\", re.IGNORECASE)\n",
        "    reg_speaker2 = re.compile(\"^(\"+pre_or+\")(\\s+?\\w+?[^\\(\\):]*?)\\(([^\\(\\):]+?)\\)\\s*?(\\([^\\(\\):]+?\\))*?\\s*?:\\s*?(.+)$\", re.IGNORECASE)\n",
        "    reg_speaker3 = re.compile(\"^([^\\(\\):]+?\\s)(\"+post_or1+\")([^\\(\\):]*?)(\\([^\\(\\):]+?\\))??\\s*?(\\([^\\(\\):]+?\\))*?\\s*?:\\s*?(.+)$\", re.IGNORECASE)\n",
        "    reg_speaker4 = re.compile(\"^([^\\(\\):]+?\\s)(\"+post_or1+\")([^\\(\\):]*?)\\(([^\\(\\):]+?)\\)\\s*?(\\([^\\(\\):]+?\\))*?\\s*?:\\s*?(.+)$\", re.IGNORECASE)\n",
        "    reg_speaker5 = re.compile(\"^(\"+positions_or+\")\\s*?(\\([^\\(\\)]+?\\))*?\\s*?:\\s*?(.+)$\", re.IGNORECASE)\n",
        "    reg_speaker6 = re.compile(\"^(mr\\. president|mr\\. president|mrs\\. president)\\s*?(\\([^\\(\\)]+?\\))*?\\s*?:\\s*?(.+)$\", re.IGNORECASE)\n",
        "\n",
        "    reg_notcountry = re.compile(r\"\\([^\\(\\):]*?(transl|interpret|spoke)[^\\(\\):]+?\\)\", re.IGNORECASE)\n",
        "\n",
        "    reg_notprocedural = re.compile(r\"(in my capacity as|in my national capacity|resume my functions as)\", re.IGNORECASE)\n",
        "\n",
        "    reg_parentheses = re.compile(r\"\\([^\\(\\)]+?\\)\", re.IGNORECASE)\n",
        "\n",
        "    reg_quote1 = re.compile(r\"“[^“”\\n]+?(”|\\n)\")\n",
        "    reg_quote2 = re.compile(r\"\\\"[^\\\"\\\"\\n]+?(\\\"|\\n)\")\n",
        "\n",
        "    reg_start = re.compile(r\"^(the meeting (was called to order|resumed|was resumed|was suspended)|held (at|in)).+$\", re.IGNORECASE)\n",
        "    reg_agenda = re.compile(r\"^the agenda was (adopted|that).+$\", re.IGNORECASE)\n",
        "    reg_end = re.compile(r\"^(the meeting (rose|was suspended)|the council (rose|suspended)).+$\", re.IGNORECASE)\n",
        "    reg_end_search = re.compile(r\"(the meeting (rose|was suspended)|the council (rose|suspended))\", re.IGNORECASE)\n",
        "    reg_sectitle = re.compile(r\"^(adoption of the agenda|expression of.+|opening statement).*$\", re.IGNORECASE)\n",
        "    reg_descript = re.compile(r\"^(there being no objection|.*?it (is|was) so decided|it (is|was) so agreed|at the invitation of the president).*$\", re.IGNORECASE)\n",
        "    reg_vote = re.compile(r\"^((a vote|vote|a secret ballot) was taken|(the proposal|the draft resolution) was (rejected|adopted)|in (favour|favor):|.*?against:|.*?abstaining:).*$\", re.IGNORECASE)\n",
        "\n",
        "    reg_space1 = re.compile(r\"\\s—\\s\")\n",
        "    reg_space2 = re.compile(r\"\\s\\s+\")\n",
        "    reg_space3 = re.compile(r\"\\s+\")\n",
        "\n",
        "    with open(path, 'r') as f:\n",
        "        txt = f.read()\n",
        "\n",
        "        to_end = len(reg_end_search.findall(txt))\n",
        "        if to_end == 0:\n",
        "            to_end = 999\n",
        "\n",
        "        paras = txt.split(\"\\n\\n\")\n",
        "        num_para = len(paras)\n",
        "        order = 0\n",
        "        para_list = []\n",
        "        procedural = False\n",
        "        speakers = []\n",
        "        countries = []\n",
        "        speeches = []\n",
        "        procedural_flags = []\n",
        "        records = []\n",
        "        breakout = False\n",
        "        for i, para in enumerate(paras):\n",
        "            para = para.strip()\n",
        "\n",
        "            if reg_speaker1.match(para) or reg_speaker3.match(para) or reg_speaker5.match(para) or reg_speaker6.match(para):\n",
        "                speech_start = False\n",
        "\n",
        "                para = reg_notcountry.sub(\"\", para)\n",
        "                para = reg_speaker6.sub(r\"The President \\2: \\3\", para)\n",
        "\n",
        "                if reg_speaker1.match(para):\n",
        "                    speaker = reg_speaker1.sub(r\"\\1\\2\", para).strip()\n",
        "                    if reg_speaker2.match(para):\n",
        "                        country = reg_speaker2.sub(r\"\\3\", para).strip()\n",
        "                    else:\n",
        "                        country = \"n.a.\"\n",
        "                    body = reg_speaker1.sub(r\"\\5\", para).strip()\n",
        "                    if is_nameonly(speaker, speaker_thres):\n",
        "                        speech_start = True\n",
        "                        para = body\n",
        "                elif reg_speaker3.match(para):\n",
        "                    speaker = reg_speaker3.sub(r\"\\1\\2\\3\", para).strip()\n",
        "                    if reg_speaker4.match(para):\n",
        "                        country = reg_speaker4.sub(r\"\\4\", para).strip()\n",
        "                    else:\n",
        "                        country = \"n.a.\"\n",
        "                    body = reg_speaker3.sub(r\"\\6\", para).strip()\n",
        "                    if is_nameonly(speaker, speaker_thres):\n",
        "                        speech_start = True\n",
        "                        para = body\n",
        "                elif reg_speaker5.match(para):\n",
        "                    speech_start = True\n",
        "                    speaker = reg_speaker5.sub(r\"\\1\", para).strip()\n",
        "                    if speaker.lower() == \"the president\":\n",
        "                        speaker = \"The President\"\n",
        "                        country = \"n.a.\"\n",
        "                    elif speaker.lower() == \"the chairman\":\n",
        "                        speaker = \"The Chairman\"\n",
        "                        country = \"n.a.\"\n",
        "                    elif speaker.lower() == \"the acting president\":\n",
        "                        speaker = \"The Acting President\"\n",
        "                        country = \"n.a.\"\n",
        "                    else:\n",
        "                        country = \"United Nations\"\n",
        "                    para = reg_speaker5.sub(r\"\\3\", para).strip()\n",
        "\n",
        "                if speech_start:\n",
        "                    order += 1\n",
        "                    speakers.append(speaker)\n",
        "                    countries.append(country)\n",
        "\n",
        "                    if order > 1:\n",
        "                        speech = \"\\n\".join([block for block in para_list if len(block) > 0])\n",
        "                        speeches.append(speech)\n",
        "                        procedural_flags.append(procedural)\n",
        "                        para_list = []\n",
        "\n",
        "                    if (speaker.lower() == \"the president\") or (speaker.lower() == \"the chairman\") or (speaker.lower() == \"the acting president\"):\n",
        "                        procedural = True\n",
        "                    else:\n",
        "                        procedural = False\n",
        "\n",
        "            if order == 0:\n",
        "                continue\n",
        "\n",
        "            if reg_end.match(para):\n",
        "                to_end -= 1\n",
        "                if to_end == 0:\n",
        "                    speech = \"\\n\".join(para_list)\n",
        "                    speeches.append(speech)\n",
        "                    procedural_flags.append(procedural)\n",
        "                    para_list = []\n",
        "                    breakout = True\n",
        "                    break\n",
        "\n",
        "            if (reg_start.match(para)) or (reg_agenda.match(para)) or (reg_end.match(para)) or (reg_sectitle.match(para)) or (reg_descript.match(para)) or (reg_vote.match(para)):\n",
        "                continue\n",
        "\n",
        "            if (speaker.lower() == \"the president\") or (speaker.lower() == \"the chairman\"):\n",
        "                if reg_notprocedural.search(reg_space2.sub(' ', para.replace('\\n', ' '))):\n",
        "                    procedural = False\n",
        "\n",
        "            if remove_parentheses:\n",
        "                para = reg_parentheses.sub(\"\", para)\n",
        "\n",
        "            if remove_quotes:\n",
        "                para = reg_space2.sub(' ', reg_space1.sub(\" \", reg_quote1.sub(\"\", para)))\n",
        "                para = reg_space2.sub(' ', reg_space1.sub(\" \", reg_quote2.sub(\"\", para)))\n",
        "\n",
        "            para = reg_space3.sub(\" \", para.strip())\n",
        "            para_list.append(para)\n",
        "\n",
        "        if not breakout:\n",
        "            speech = \"\\n\".join(para_list)\n",
        "            speeches.append(speech)\n",
        "            procedural_flags.append(procedural)\n",
        "            para_list = []\n",
        "\n",
        "        if order == 0:\n",
        "            print(\"no speaker\", path)\n",
        "            return None\n",
        "\n",
        "        num_speakers = len(speakers)\n",
        "        num_speeches = len(speeches)\n",
        "\n",
        "        if num_speakers != num_speeches:\n",
        "            print(\"speaker-speech discrepancies\", num_speakers, num_speeches, path)\n",
        "            return None\n",
        "\n",
        "        counter = 1\n",
        "        for i in range(num_speakers):\n",
        "            speaker = speakers[i]\n",
        "            country = countries[i]\n",
        "            speech = speeches[i]\n",
        "            count = len(speech.split())\n",
        "            if count == 0:\n",
        "                continue\n",
        "            procedural = procedural_flags[i]\n",
        "            row = [counter, speaker, country, speech, count, procedural]\n",
        "            records.append(row)\n",
        "            counter += 1\n",
        "\n",
        "    return records"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUL-cvXodTXn"
      },
      "outputs": [],
      "source": [
        "# Extract text from each file and process speaker/speech data\n",
        "import os\n",
        "\n",
        "# Options\n",
        "save = True\n",
        "speaker_thres = 5\n",
        "remove_parentheses = True\n",
        "remove_quotes = False\n",
        "\n",
        "# Set the output directory for extracted speech texts\n",
        "out_dir = r\"PATH_TO_SPEECH_OUTPUT\"  # Replace with your folder path for speech output files\n",
        "tag_in = \"_clean\"\n",
        "tag_out = \"_extracted\"\n",
        "\n",
        "# List with additional information for Secretary General (start year and name)\n",
        "SG_list = [(2017, 'Mr. Guterres'), (2007, 'Mr. Ban'), (1997, 'Mr. Annan'), (1992, 'Mr. Boutros-Ghali'),\n",
        "          (1982, 'Mr. Perez de Cuellar'), (1972, 'Mr. Waldheim'), (1961, 'U Thant'),\n",
        "          (1953, 'Mr. Hammarskjöld'), (1946, 'Mr. Lie')]\n",
        "\n",
        "num_docs = df_cleaned.shape[0]\n",
        "\n",
        "# Filter if needed\n",
        "filt = [True] * num_docs\n",
        "df_select = df_cleaned[filt]\n",
        "\n",
        "speech_flags = []\n",
        "data = []\n",
        "for i in range(num_docs):\n",
        "    if i % 100 == 0:\n",
        "        print(\"Preprocessing Doc {} (out of {})...\".format(i+1, num_docs))\n",
        "\n",
        "    # Get meeting information\n",
        "    series = df_select.iloc[i]\n",
        "    record_id = series[\"record_id\"]\n",
        "    doc_name = series[\"doc_name\"]\n",
        "    meeting_num = series[\"meeting_num\"]\n",
        "    year = series[\"year\"]\n",
        "    month = series[\"month\"]\n",
        "    day = series[\"day\"]\n",
        "    topic = series[\"topic\"]\n",
        "    agenda = series[\"agenda\"]\n",
        "    pres_name = series[\"pres_name\"]\n",
        "    pres_country = series[\"pres_country\"]\n",
        "    path = series['path']\n",
        "\n",
        "    # Extract speech records\n",
        "    records = speech_extraction(path, speaker_thres=speaker_thres, remove_parentheses=remove_parentheses,\n",
        "                                remove_quotes=remove_quotes)\n",
        "\n",
        "    if records is None:\n",
        "        speech_flags.append(False)\n",
        "        continue\n",
        "\n",
        "    for record in records:\n",
        "        order = record[0]\n",
        "        speaker = record[1]\n",
        "        country = record[2]\n",
        "        speech = record[3]\n",
        "        count = record[4]\n",
        "        procedural = record[5]\n",
        "        president = False\n",
        "        secretary_general = False\n",
        "\n",
        "        speech_id = record_id + \"-\" + str(order).zfill(3)\n",
        "\n",
        "        if (speaker.lower() == \"the president\") or (speaker.lower() == \"the acting president\"):\n",
        "            president = True\n",
        "            speaker = str(pres_name) if pres_name not in [\"\", \"nan\"] else \"n.a.\"\n",
        "            country = str(pres_country) if pres_country not in [\"\", \"nan\"] else \"n.a.\"\n",
        "        elif speaker.lower().startswith(\"the secretary\") and speaker.lower().endswith(\"general\"):\n",
        "            secretary_general = True\n",
        "            for start, person in SG_list:\n",
        "                if year >= start:\n",
        "                    speaker = person\n",
        "                    break\n",
        "            country = \"United Nations\"\n",
        "        elif speaker.lower() == \"the chairman\":\n",
        "            president = True\n",
        "            speaker = \"Mr. N. J. O. MAKIN\"\n",
        "            country = \"Australia\"\n",
        "\n",
        "        row = [speech_id, record_id, doc_name, meeting_num, year, month, day, topic, agenda, order, speaker, country, president, secretary_general, procedural, count, speech]\n",
        "        data.append(row)\n",
        "    speech_flags.append(True)\n",
        "\n",
        "    if save:\n",
        "        out_file = os.path.basename(path).replace(tag_in, tag_out)\n",
        "        out_path = os.path.join(out_dir, out_file)\n",
        "        with open(out_path, \"w\") as f:\n",
        "            for record in records:\n",
        "                line1 = \"{}, {}, {}, {}\\n\".format(record[0], record[1], record[2], record[-1])\n",
        "                line2 = record[3] + \"\\n\\n\"\n",
        "                f.write(line1)\n",
        "                f.write(line2)\n",
        "\n",
        "print('{} records have been processed.'.format(len(speech_flags)))\n",
        "print('{} speeches have been extracted.'.format(len(data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K52kMgeNdTXn"
      },
      "outputs": [],
      "source": [
        "# Create a DataFrame from the extracted speech data\n",
        "labels = [\"speech_id\", \"record_id\", \"doc_name\", \"meeting_num\", \"year\", \"month\", \"day\", \"topic\", \"agenda\",\n",
        "          \"order\", \"speaker\", \"country\", \"president\", \"secretary_general\", \"procedural\", \"count\", \"speech\"]\n",
        "df_speeches = pd.DataFrame(data, columns=labels)\n",
        "df_speeches.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-PxulBIdTXn"
      },
      "outputs": [],
      "source": [
        "# Check the shape of the speeches DataFrame\n",
        "df_speeches.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXX-9aOsdTXo"
      },
      "outputs": [],
      "source": [
        "# Add a flag in the document list indicating availability of speech data\n",
        "df_select[\"speech_data\"] = speech_flags\n",
        "df_select.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PBrq0ugdTXo"
      },
      "outputs": [],
      "source": [
        "# Check the shape of the updated document list\n",
        "df_select.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qT7H29BTdTXo"
      },
      "outputs": [],
      "source": [
        "# Count the number of transcripts with no speech data\n",
        "(df_select.speech_data == False).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZ3XnUd9dTXo"
      },
      "outputs": [],
      "source": [
        "# Save the new speech data\n",
        "out_path = r\"PATH_TO_OUTPUT_DF/speeches_new.tsv\"  # Replace with your output path for new speeches\n",
        "df_speeches.to_csv(out_path, sep='\\t', na_rep='NULL')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_mnsjdLdTXo"
      },
      "outputs": [],
      "source": [
        "# Save the updated document list\n",
        "out_path = r\"PATH_TO_OUTPUT_DF/UNSC_PV_cleaned.csv\"\n",
        "df_select.to_csv(out_path, na_rep='NULL')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncKBjcSedTXo"
      },
      "source": [
        "### Reflecting Speech Data Availability in the Meetings Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQC7dzFAdTXo"
      },
      "outputs": [],
      "source": [
        "# Reload the updated document list\n",
        "import pandas as pd\n",
        "\n",
        "in_path = r\"PATH_TO_OUTPUT_DF/UNSC_PV_cleaned.csv\"\n",
        "df_cleaned = pd.read_csv(in_path, na_values='NULL', index_col=0)\n",
        "df_cleaned.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pj9DoZHdTXo"
      },
      "outputs": [],
      "source": [
        "# Reload the meetings data\n",
        "in_path = r\"PATH_TO_MEETINGS_FILE/meetings.tsv\"\n",
        "df_meetings = pd.read_csv(in_path, na_values='NULL', index_col=0, sep='\\t')\n",
        "df_meetings.tail(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wV6_bDrdTXo"
      },
      "outputs": [],
      "source": [
        "# Update the meetings data with the speech_data flag from the document list\n",
        "import numpy as np\n",
        "\n",
        "to_skip = [\"S_PV.2977(PartI)-EN\", \"S_PV.2977(PartII)(closed)-EN\", \"S_PV.2977(PartII)(closed-resumption1)-EN\",\n",
        "           \"S_PV.2977(PartII)(closed-resumption2)-EN\", \"S_PV.2977(PartII)(closed-resumption3)-EN\",\n",
        "           \"S_PV.2977(PartII)(closed-resumption4)-EN\", \"S_PV.2977(PartII)(closed-resumption5)-EN\", \"S_PV.3160_RU\"]\n",
        "\n",
        "num_docs = df_cleaned.shape[0]\n",
        "counter = 0\n",
        "for i in range(num_docs):\n",
        "    if i % 100 == 0:\n",
        "        print(\"Processing document {}/{}\".format(i+1, num_docs))\n",
        "    series = df_cleaned.iloc[i]\n",
        "    meeting_num = series[\"meeting_num\"]\n",
        "    doc_name = series[\"doc_name\"]\n",
        "    speech_data = series[\"speech_data\"]\n",
        "    if doc_name in to_skip:\n",
        "        continue\n",
        "    select = (df_meetings[\"meeting_num\"] == meeting_num)\n",
        "    if select.sum() > 1:\n",
        "        print(\"multiple matches\", doc_name)\n",
        "        df_meetings_select = df_meetings[select]\n",
        "        select2 = (df_meetings_select[\"record\"] == doc_name.replace(\"_\", \"/\"))\n",
        "        if select2.sum() == 0:\n",
        "            print(\"in the end, no match\", doc_name)\n",
        "        else:\n",
        "            match = df_meetings_select[select2].iloc[0]\n",
        "            df_meetings.loc[match.name, \"speeches\"] = speech_data\n",
        "            counter += 1\n",
        "    elif select.sum() == 0:\n",
        "        print(\"no match\", doc_name)\n",
        "    else:\n",
        "        match = df_meetings[select].iloc[0]\n",
        "        df_meetings.loc[match.name, \"speeches\"] = speech_data\n",
        "        counter += 1\n",
        "print(\"{} rows have been updated.\".format(counter))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIhkChBDdTXo"
      },
      "outputs": [],
      "source": [
        "# Check the updated meetings data\n",
        "df_meetings.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWcTV2pNdTXp"
      },
      "outputs": [],
      "source": [
        "df_meetings.speeches.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXqH8kFtdTXp"
      },
      "outputs": [],
      "source": [
        "# Sort the meetings data in ascending order by record_id\n",
        "df_meetings_sorted = df_meetings.sort_values(\"record_id\")\n",
        "df_meetings_sorted.reset_index(drop=True, inplace=True)\n",
        "df_meetings_sorted.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYi5-ZVadTXp"
      },
      "outputs": [],
      "source": [
        "df_meetings_sorted.tail(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lzi91g2KdTXp"
      },
      "outputs": [],
      "source": [
        "df_meetings_sorted.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggFuK6y-dTXp"
      },
      "outputs": [],
      "source": [
        "# Save the sorted meetings data\n",
        "out_path = r\"PATH_TO_MEETINGS_FILE/meetings.tsv\"\n",
        "df_meetings_sorted.to_csv(out_path, sep='\\t', na_rep='NULL')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "hGiaLw4zdTXp",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "source": [
        "### Merging Speech Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OodRZF9wdTXp"
      },
      "outputs": [],
      "source": [
        "# Load the old speech data\n",
        "import pandas as pd\n",
        "\n",
        "# Replace with the old speeches file path\n",
        "in_path = r\"PATH_TO_OLD_SPEECHES/speeches_old.tsv\"\n",
        "df_speeches1 = pd.read_csv(in_path, na_values='NULL', index_col=0, sep='\\t')\n",
        "df_speeches1.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmPFpu0OdTXp"
      },
      "outputs": [],
      "source": [
        "df_speeches1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lr6PHOQQdTXp"
      },
      "outputs": [],
      "source": [
        "df_speeches1.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaClvz_MdTXp"
      },
      "outputs": [],
      "source": [
        "# Load the new speech data\n",
        "in_path = r\"PATH_TO_OUTPUT_DF/speeches_new.tsv\"\n",
        "df_speeches2 = pd.read_csv(in_path, na_values='NULL', index_col=0, sep='\\t')\n",
        "df_speeches2.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Dapj5KAdTXp"
      },
      "outputs": [],
      "source": [
        "df_speeches2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BoD7k6vdTXp"
      },
      "outputs": [],
      "source": [
        "# Concatenate and sort the old and new speech data\n",
        "speeches_combined = pd.concat([df_speeches1, df_speeches2], ignore_index=True)\n",
        "speeches_combined.sort_values(['speech_id', 'year', 'month', 'day'], inplace=True)\n",
        "speeches_combined.reset_index(inplace=True, drop=True)\n",
        "speeches_combined.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38gZoopIdTXp"
      },
      "outputs": [],
      "source": [
        "speeches_combined.tail(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dY_z-Sv-dTXq"
      },
      "outputs": [],
      "source": [
        "# Check the shape of the combined speeches data\n",
        "speeches_combined.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPu3Df7vdTXq"
      },
      "outputs": [],
      "source": [
        "# Save the combined speech data\n",
        "out_path = r\"PATH_TO_SPEECH_OUTPUT/speeches.tsv\"\n",
        "speeches_combined.to_csv(out_path, sep='\\t', na_rep='NULL')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeaVTRwldTXq"
      },
      "outputs": [],
      "source": [
        "len(speeches_combined.country.unique())"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
